# -*- coding: utf-8 -*-
"""PY_lab02

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ulbBqSU9yuxB2vaabaYBi53D5MFydct5

**EX1**

**Build Classification of Students Academic Performance**

**Dataset:** The dataset consists of 480 student records and 16 features. The features are classified into three major categories: (1) Demographic features such as gender and nationality. (2) Academic background features such as educational stage, grade Level and section. (3) Behavioral features such as raised hand on class, opening resources, answering survey by parents, and school satisfaction.

**The students are classified into three numerical intervals based on their total grade/mark:**
Low-Level (L): interval includes values from 0 to 69
Middle-Level (M): interval includes values from 70 to 89
High-Level (H): interval includes values from 90-100.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
import os

df = pd.read_csv("xAPI-Edu-Data.csv")
df.head(50)

df.shape

df.columns

df.rename(index=str, columns={'gender':'Gender', 
                               'NationalITy':'Nationality',
                              'raisedhands':'RaisedHands',
                              'VisITedResources':'VisitedResources'},inplace=True)
df.columns

print("Class Unique Values: ", df["Class"].unique())
print("Topic Unique Values: ", df["Topic"].unique())
print("StudentAbsenceDays Unique Values: ", df["StudentAbsenceDays"].unique())
print("ParentschoolSatisfaction Unique Values: ", df["ParentschoolSatisfaction"].unique())
print("Relation Unique Values: ", df["Relation"].unique())
print("SectionID Unique Values: ", df["SectionID"].unique())
print("Gender Unique Values: ", df["Gender"].unique())

"""**Exploratory data analysis (EDA)**
*   Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.
*   It is a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand,before getting them dirty with it.
"""

for i in range(1,17):
  print(df.iloc[:,i].value_counts())
  print("*"*20)

sns.pairplot(df,hue='Class')

plt.figure(figsize=(14,12))
sns.heatmap(df.corr(), linewidths=.1, cmap="YlGnBu", annot=True)
plt.yticks(rotation=0)
#fetures A và B có tương quan cao (0.69), kiểm tra lại mối tương quan của 2 features này để thực hiện fetures selection

"""**Exploring 'Class' label**"""

print(df.groupby('Class').size())

P_Satis = sns.countplot(x="Class",data=df,linewidth=2,edgecolor=sns.color_palette("dark"))
#dữ liệu cân bằng ngoại trừ M.ta sẽ thực hiên M

df.Class.value_counts(normalize=True).plot(kind='bar')

"""**Exploring 'RaisedHands' feature**"""

#Histogram of raisedhands
plt.subplots(figsize=(15 ,5))
df["RaisedHands"].value_counts().sort_index().plot.bar()
plt.title("No. of times vs no. of students raised their hands on particular time", fontsize=18)
plt.xlabel("No. of times, student raised their hands", fontsize=14)
plt.ylabel("No. of student, on particular times", fontsize=14)
plt.show()

Raised_hand = sns.boxplot(x="Class", y="RaisedHands", data=df)
plt.show()

Facetgrid = sns.FacetGrid(df,hue='Class',size=6)    #Initialize the grid
Facetgrid.map(sns.kdeplot,'RaisedHands',shade=True) #Apply a plotting function to each facet’s subset of the data
Facetgrid.set(xlim=(0,df['RaisedHands'].max()))     #Set attributes on each subplot Axes.
Facetgrid.add_legend()                              #Add legend on Seaborn facetgrid plot

df.groupby(['ParentschoolSatisfaction'])['Class'].value_counts()

#Pie Chart
labels=df.ParentschoolSatisfaction.value_counts()
colors=["blue","green"]
explode=[0,0]
sizes=df.ParentschoolSatisfaction.value_counts().values
plt.figure(figsize = (7,7))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')
plt.title("Parent school Satisfaction in Data",fontsize = 15)
plt.show()

#Statistical Summary:
df.describe()
feature_names = ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']
X = df[feature_names]
y = df['Class']

# scaling to the test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**Build Models**"""

#K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
print('Accuracy of K-NN classifier on training set: {:.2f}'
     .format(knn.score(X_train, y_train)))
print('Accuracy of K-NN classifier on test set: {:.2f}'
     .format(knn.score(X_test, y_test)))

#Logistic Regression
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
print('Accuracy of Logistic regression classifier on training set: {:.2f}'
     .format(logreg.score(X_train, y_train)))
print('Accuracy of Logistic regression classifier on test set: {:.2f}'
     .format(logreg.score(X_test, y_test)))

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier().fit(X_train, y_train)
print('Accuracy of Decision Tree classifier on training set: {:.2f}'
     .format(clf.score(X_train, y_train)))
print('Accuracy of Decision Tree classifier on test set: {:.2f}'
     .format(clf.score(X_test, y_test)))

#Linear Discriminant Analysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
print('Accuracy of LDA classifier on training set: {:.2f}'
     .format(lda.score(X_train, y_train)))
print('Accuracy of LDA classifier on test set: {:.2f}'
     .format(lda.score(X_test, y_test)))

#Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
print('Accuracy of GNB classifier on training set: {:.2f}'
     .format(gnb.score(X_train, y_train)))
print('Accuracy of GNB classifier on test set: {:.2f}'
     .format(gnb.score(X_test, y_test)))

#Support Vector Machine
from sklearn.svm import SVC
svm = SVC()
svm.fit(X_train, y_train)
print('Accuracy of SVM classifier on training set: {:.2f}'
     .format(svm.score(X_train, y_train)))
print('Accuracy of SVM classifier on test set: {:.2f}'
     .format(svm.score(X_test, y_test)))

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
pred = clf.predict(X_test)
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))

"""**EX2 The New York City Airbnb**"""

# Commented out IPython magic to ensure Python compatibility.
#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline
import seaborn as sns

#read Data
airbnb=pd.read_csv("AB_NYC_2019.csv")
airbnb.head(10)

airbnb.shape

airbnb.columns

print("1.Name Unique Values: ", airbnb["name"].unique())
print("2.host_Name Unique Values: ", airbnb["host_name"].unique())
print("3.neighbourhood_group Unique Values: ", airbnb["neighbourhood_group"].unique())
print("4.room_type Unique Values: ", airbnb["room_type"].unique())

#checking type of every column in the dataset
airbnb.dtypes

"""**Understadning, Wrangling and Cleaning Data**"""

#looking to find out first what columns have null values
airbnb.isnull().sum()

"""**Nhận xét:**các cột "name","host_name" không quan trọng, các cột "last_review" và"review_per_month" "review_per_month" cần xử lý bằng cách thêm 0 cho các giá trị bị thiếu là được."""

airbnb.drop(['id','host_name','last_review'], axis=1, inplace=True)
airbnb.head(3)

#replacing all NaN values in 'reviews_per_month' with 0
airbnb.fillna({'reviews_per_month':0}, inplace=True)
airbnb.reviews_per_month.isnull().sum()

#kiễm tra cột n_group
airbnb.neighbourhood_group.unique()

#kiễm tra chiều dài của neighbourhood
len(airbnb.neighbourhood.unique())

#kiễm tra room_type
airbnb.room_type.unique()

"""**Exploring and Visualizing Data**"""

# xử lý cột host id xem id nào có nhiều trong danh sách nhât
top_host=airbnb.host_id.value_counts().head(10)
top_host

#setting figure size for future visualizations
sns.set(rc={'figure.figsize':(10,8)})
sns.set_style('white')
top_host_df=pd.DataFrame(top_host)
top_host_df.reset_index(inplace=True)
top_host_df.rename(columns={'index':'Host_ID', 'host_id':'P_Count'}, inplace=True)
top_host_df

viz_1=sns.barplot(x="Host_ID", y="P_Count", data=top_host_df,
                 palette='Blues_d')
viz_1.set_title('Hosts with the most listings in NYC')
viz_1.set_ylabel('Count of listings')
viz_1.set_xlabel('Host IDs')
viz_1.set_xticklabels(viz_1.get_xticklabels(), rotation=45)
# dựa trên biểu đồ ta thấy đây 10 giá trị đầu tiên trong host phân bố khá hợp lí

# tiềm hiểu về cột neiberhoods trích giá của nó
#Brooklyn
sub_1=airbnb.loc[airbnb['neighbourhood_group'] == 'Brooklyn']
price_sub1=sub_1[['price']]
#Manhattan
sub_2=airbnb.loc[airbnb['neighbourhood_group'] == 'Manhattan']
price_sub2=sub_2[['price']]
#Queens
sub_3=airbnb.loc[airbnb['neighbourhood_group'] == 'Queens']
price_sub3=sub_3[['price']]
#Staten Island
sub_4=airbnb.loc[airbnb['neighbourhood_group'] == 'Staten Island']
price_sub4=sub_4[['price']]
#Bronx
sub_5=airbnb.loc[airbnb['neighbourhood_group'] == 'Bronx']
price_sub5=sub_5[['price']]
#putting all the prices' dfs in the list
price_list_by_n=[price_sub1, price_sub2, price_sub3, price_sub4, price_sub5]

#creating an empty list that we will append later with price distributions for each neighbourhood_group
p_l_b_n_2=[]
#creating list with known values in neighbourhood_group column
nei_list=['Brooklyn', 'Manhattan', 'Queens', 'Staten Island', 'Bronx']
#creating a for loop to get statistics for price ranges and append it to our empty list
for x in price_list_by_n:
    i=x.describe(percentiles=[.25, .50, .75])
    i=i.iloc[3:]
    i.reset_index(inplace=True)
    i.rename(columns={'index':'Stats'}, inplace=True)
    p_l_b_n_2.append(i)
#changing names of the price column to the area name for easier reading of the table    
p_l_b_n_2[0].rename(columns={'price':nei_list[0]}, inplace=True)
p_l_b_n_2[1].rename(columns={'price':nei_list[1]}, inplace=True)
p_l_b_n_2[2].rename(columns={'price':nei_list[2]}, inplace=True)
p_l_b_n_2[3].rename(columns={'price':nei_list[3]}, inplace=True)
p_l_b_n_2[4].rename(columns={'price':nei_list[4]}, inplace=True)
#finilizing our dataframe for final view    
stat_df=p_l_b_n_2
stat_df=[df.set_index('Stats') for df in stat_df]
stat_df=stat_df[0].join(stat_df[1:])
stat_df

#Statistical Summary:
airbnb.describe()
feature_names = ['minimum_nights', 'number_of_reviews', 'calculated_host_listings_count', 'availability_365']
X = airbnb[feature_names]
y = airbnb['price']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
print('Accuracy of Logistic regression classifier on training set: {:.2f}'
     .format(logreg.score(X_train, y_train)))
print('Accuracy of Logistic regression classifier on test set: {:.2f}'
     .format(logreg.score(X_test, y_test)))